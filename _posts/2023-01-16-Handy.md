---
layout: post
title: Introducing Handy&#58; Our Open Source Motion Capture Tool
subtitle: Handy
video: https://cdn.shopify.com/videos/c/o/v/4dc8b77fc2514b3ba450aaa1e206e788.mp4
---

![_config.yml]({{ site.baseurl }}/images/handy/handy_logo.png){:.centered}

Today we wanted to share a tool we are open sourcing called [handy](https://github.com/Shopify/handy). This tool can be used to motion-capture your hands and head while you wear a Meta Quest headset.

You can see a live demo of data captured with handy [here](https://diegomacario.github.io/Hands-In-The-Web/public/index.html).

It currently outputs [Alembic](https://www.alembic.io/) files, which you can import into tools like Blender to create animations, or into game engines like Unity and Unreal.

We have used handy internally to create concept videos like the [Matrix Stockroom]({{ site.baseurl }}{% link _posts/2023-01-17-Matrix-Stockroom.md %}). We worked hard to make this tool super easy to use, so if you are an artist without a strong technical background, don't worry, handy is made for you.

The way it works is quite simple:

- When you start recording, the transforms of the joints of the hands and of the head start being saved into a JSON file.
- When you stop recording, that JSON file is wirelessly sent from your headset to the Unity editor.
- When the Unity editor receives it, it reads the transforms that it contains and replays them to generate an Alembic file.

While developing this tool we were surprised by how simple the Oculus hands are. They are just standard rigged meshes with 18 relevant bones each.

![_config.yml]({{ site.baseurl }}/images/handy/hand_bones.png){:.centered}

We are excited to see what you'll do with handy! To start using it visit its Github [repository](https://github.com/Shopify/handy).
