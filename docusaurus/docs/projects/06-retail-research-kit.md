---
id: retail-research-kit
title: Retail Research Kit
slug: /projects/retail-research-kit
---

# Retail Research Kit

During the course of our last project, the [Meta Quest Pro](https://www.meta.com/quest/quest-pro/) launched, which brought with it a new mix of capabilities that hasn't been brought together before: a portable VR headset, with hand and eye tracking, and color passthrough for mixed reality development. We wondered: what new use cases would this headset unlock for merchants?

<iframe width="960" height="960" src="/heatmap-visualization/" />

## Solution focused on small businesses

We realized that merchants who want to run user research studies in their stores right now have many professional options, but these studies can be expensive and time-consuming, and frankly they seem like a daunting undertaking. This is one of those areas where larger businesses are able to afford the time and money costs of these studies and optimize, putting small businesses under even more pressure to compete. We thought - hey, it might be possible to put together a retail research kit that merchants could rent for cheap (or free.) Could we use this new tech to give small businesses the tools they need to compete?

![Meta Quest Pro](/img/retail-research-kit/quest-pro.jpeg)

### Software preparation

First, we needed some software that could connect to the headset and request the user's position and orientation, and do the same for each hand's position and orientation, and the same for the gaze. After some trial-and-error, we found that at the time, Unity 2022.1.21f1 and the Built-In render pipeline was the way to go for Quest Pro (however now you can use 2022.2.1f1 and URP!) We coded up a script to save these position and orientation values over time, and another script to read those saved values and run a playback in the editor.

![Playback Video](/img/placeholder.png)

### Software preparation

The key is that all these things, the gaze, the hand position, and the user's position within the store - these are all tied together, and can be used to create secondary artifacts like heatmaps and reports. But in order to do so, you need all of this information to be correct, so when the user looks at a product, we know the right place to mark down on the heatmap.

The easy part is the hand and eye tracking, and even the user's position in space: this is all provided by the headset and is aligned together. The hard part is aligning all of this with the real world, such that the walls and floors and products really match up. To accomplish this we created a configuration mode where you load a pre-scanned 3D model of the environment, and drag it into alignment with the real environment using the controller and passthrough cameras.

![Alignment Adjustment Video](/img/placeholder.png)

### In-store trial (and error)

With all of this working, we were ready to give this idea a trial run! We offered some compensation for people's time, called local Shopify stores in our area, and we found that a local store named [Paxton Gate](https://paxtongate.com/) was as eager as we were to find out more. We'd have to scan the store first into a 3D model, then load that into our research tools so that we could do the alignment and begin recording.

Then, since we really needed that scan to work before the rest could be aligned, of course the store failed to scan. We cycled through different phones, Android and iPhone, and it just turned out that scanning the whole store was too much for the software that we had based our workflow around.

![Scan Fail Video](/img/placeholder.png)

We discovered that by focusing on a smaller part of the store, we could get a scan that would complete, and we were able to use that scan for the remainder of the research. We found, after one participant had left, that for the majority the time their eyes were not tracked. So it's important to acknowledge the reality of the current state of the art, while understanding that this is brand new technology that just launched.

## Takeaways

When it's all working, it's invaluable and magical! All using technology that's within reach for small business merchants today. We now have an interactive heatmap of the items that caught people's attention in the store, generated by our own researchers in our own environment. For a merchant who's never had easy access to this type of technology, this is now a huge opportunity.

